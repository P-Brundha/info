{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmZm5Gq467DyM4ST0qVxcz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/P-Brundha/info/blob/main/23BIT012_In_memory_data_processing_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtS0c1nwx8wi",
        "outputId": "178a4d1d-a777-4112-a73f-a589e9f56681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Spark session initialized successfully\n",
            "‚úÖ Generated synthetic dataset with 1,000,000 rows\n",
            "üì¶ Spark DataFrame created and ready for processing\n",
            "+--------+------------------+--------------------+\n",
            "| segment|           avg_txn|           total_txn|\n",
            "+--------+------------------+--------------------+\n",
            "|Platinum|505.24028937682357|1.2600591769000104E8|\n",
            "|  Silver|504.36755270173677|1.2599202339999925E8|\n",
            "|    Gold| 503.7946137469375| 1.261269967300008E8|\n",
            "|  Bronze|505.38293141036445|1.2657113364000013E8|\n",
            "+--------+------------------+--------------------+\n",
            "\n",
            "üïí Query time (without caching): 12.84 sec\n",
            "üíæ Data cached in memory successfully\n",
            "+--------+------------------+--------------------+\n",
            "| segment|           avg_txn|           total_txn|\n",
            "+--------+------------------+--------------------+\n",
            "|Platinum|505.24028937682357|1.2600591769000104E8|\n",
            "|  Silver|504.36755270173677|1.2599202339999925E8|\n",
            "|    Gold| 503.7946137469375| 1.261269967300008E8|\n",
            "|  Bronze|505.38293141036445|1.2657113364000013E8|\n",
            "+--------+------------------+--------------------+\n",
            "\n",
            "‚ö° Query time (with in-memory caching): 3.87 sec\n",
            "\n",
            "üèÜ Identifying Top 5 Customers by Total Spend...\n",
            "+-------+------------------+\n",
            "|cust_id|       total_spent|\n",
            "+-------+------------------+\n",
            "|  13220|          13801.73|\n",
            "|   8593|          12962.59|\n",
            "|    937|          12920.62|\n",
            "|  24619|          12908.26|\n",
            "|  82520|12747.130000000001|\n",
            "+-------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "üßÆ Execution time (top customers): 3.05 sec\n",
            "\n",
            "‚úÖ Spark session terminated cleanly\n",
            "üìä Summary:\n",
            " - Compared runtime with and without in-memory caching\n",
            " - Demonstrated Spark DataFrame caching impact on performance\n",
            " - Extracted top 5 high-value customers successfully\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ‚ö° In-Memory vs Disk-Based Computation using Apache Spark\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, sum as _sum\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 1: Start Spark Session\n",
        "# ------------------------------------------------------------\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"MemoryPerformanceDemo\")\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"üöÄ Spark session initialized successfully\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 2: Generate Synthetic Transaction Dataset\n",
        "# ------------------------------------------------------------\n",
        "num_rows = 1_000_000  # 1 million records\n",
        "\n",
        "transactions = pd.DataFrame({\n",
        "    \"cust_id\": [random.randint(1, 120000) for _ in range(num_rows)],\n",
        "    \"txn_value\": [round(random.uniform(10, 1000), 2) for _ in range(num_rows)],\n",
        "    \"segment\": [random.choice([\"Gold\", \"Silver\", \"Bronze\", \"Platinum\"]) for _ in range(num_rows)],\n",
        "    \"customer_age\": [random.randint(18, 70) for _ in range(num_rows)]\n",
        "})\n",
        "\n",
        "print(f\"‚úÖ Generated synthetic dataset with {len(transactions):,} rows\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 3: Create Spark DataFrame\n",
        "# ------------------------------------------------------------\n",
        "df = spark.createDataFrame(transactions)\n",
        "print(\"üì¶ Spark DataFrame created and ready for processing\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 4: Run Aggregation (Disk-Based)\n",
        "# ------------------------------------------------------------\n",
        "start_time = time.time()\n",
        "summary_disk = (\n",
        "    df.groupBy(\"segment\")\n",
        "      .agg(avg(\"txn_value\").alias(\"avg_txn\"), _sum(\"txn_value\").alias(\"total_txn\"))\n",
        ")\n",
        "summary_disk.show(4)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"üïí Query time (without caching): {end_time - start_time:.2f} sec\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 5: Cache DataFrame in Memory\n",
        "# ------------------------------------------------------------\n",
        "df.cache()\n",
        "df.count()  # Force cache evaluation\n",
        "print(\"üíæ Data cached in memory successfully\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 6: Run Aggregation (In-Memory)\n",
        "# ------------------------------------------------------------\n",
        "start_time = time.time()\n",
        "summary_mem = (\n",
        "    df.groupBy(\"segment\")\n",
        "      .agg(avg(\"txn_value\").alias(\"avg_txn\"), _sum(\"txn_value\").alias(\"total_txn\"))\n",
        ")\n",
        "summary_mem.show(4)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"‚ö° Query time (with in-memory caching): {end_time - start_time:.2f} sec\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 7: Additional Analytics - Top Customers\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nüèÜ Identifying Top 5 Customers by Total Spend...\")\n",
        "start_time = time.time()\n",
        "\n",
        "top_spenders = (\n",
        "    df.groupBy(\"cust_id\")\n",
        "      .agg(_sum(\"txn_value\").alias(\"total_spent\"))\n",
        "      .orderBy(col(\"total_spent\").desc())\n",
        ")\n",
        "top_spenders.show(5)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"üßÆ Execution time (top customers): {end_time - start_time:.2f} sec\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 8: Wrap Up\n",
        "# ------------------------------------------------------------\n",
        "spark.stop()\n",
        "print(\"\\n‚úÖ Spark session terminated cleanly\")\n",
        "print(\"üìä Summary:\")\n",
        "print(\" - Compared runtime with and without in-memory caching\")\n",
        "print(\" - Demonstrated Spark DataFrame caching impact on performance\")\n",
        "print(\" - Extracted top 5 high-value customers successfully\")\n"
      ]
    }
  ]
}